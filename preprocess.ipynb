{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from GEOTIFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Hima files...\n",
      "Indexing ERA5 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AWS files: 100%|██████████| 2807/2807 [1:19:47<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   timestamp  row  col  aws_value        B09B       VSB        B16B      B06B  \\\n",
      "0 2019-04-01    3  115        0.2  252.925888  0.092611  268.228333  0.038708   \n",
      "1 2019-04-01    5  114        0.8  252.382706  0.063253  266.749786  0.030103   \n",
      "2 2019-04-01    6  116        0.6  250.753403  0.094564  261.886749  0.055151   \n",
      "3 2019-04-01    7  115       19.0  250.763870  0.198283  260.937897  0.059070   \n",
      "4 2019-04-01   15  115        1.2  250.872086  0.147400  265.053253  0.057507   \n",
      "\n",
      "         B14B         I4B  ...      U850      V250       R250        EWSS  \\\n",
      "0  282.099701  283.889587  ... -3.606674  4.523453  33.599831 -641.113281   \n",
      "1  280.077759  281.936829  ... -3.606674  4.523453  33.599831 -641.113281   \n",
      "2  272.486755  277.685822  ... -3.260971  3.843765  20.486549 -171.113281   \n",
      "3  270.996948  275.486755  ... -5.014877  3.968765  21.638893 -585.113281   \n",
      "4  278.369141  283.916656  ... -5.737534  4.207047  13.377175 -354.113281   \n",
      "\n",
      "      SSHF      TCLW       U250       TCWV        TCW      SLHF  \n",
      "0 -11962.0  0.723206  25.793488  48.219376  49.239563 -127783.0  \n",
      "1 -11962.0  0.723206  25.793488  48.219376  49.239563 -127783.0  \n",
      "2   2822.0  0.624878  29.561066  47.922501  48.798157 -132583.0  \n",
      "3 -42874.0  0.810242  27.691925  46.371719  47.450500  -71463.0  \n",
      "4 -43642.0  0.776672  28.754425  45.834610  46.835266  -57959.0  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_timestamp_from_filename_hima(file_name):\n",
    "    \"\"\"Extract the timestamp with date and hour from the Hima filename.\"\"\"\n",
    "    parts = file_name.split('_')\n",
    "    timing = parts[1]\n",
    "    subparts = timing.split('.Z')\n",
    "\n",
    "    date_str = subparts[0]  # YYYYmmdd\n",
    "    hour_str = subparts[1]  # HHMM in ZHHMM format\n",
    "    \n",
    "    timestamp = datetime.strptime(date_str + hour_str, '%Y%m%d%H%M')\n",
    "    return timestamp\n",
    "\n",
    "def parse_timestamp_from_filename_era5_aws(file_name):\n",
    "    \"\"\"Extract timestamp from ERA5 and AWS filenames.\"\"\"\n",
    "    parts = file_name.split('_')\n",
    "    date_str = parts[1].split('.')[0]  # YYYYmmddHHMMSS format\n",
    "    timestamp = datetime.strptime(date_str, '%Y%m%d%H%M%S')\n",
    "    return timestamp\n",
    "\n",
    "def extract_image_data(file_path):\n",
    "    \"\"\"Extract the 90x250 pixel array from a .tif file.\"\"\"\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data = src.read(1)  # Read the first band\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return np.full((90, 250), np.nan)  # NaN for missing data\n",
    "\n",
    "def extract_band_or_variable_name(file_path, root_dir):\n",
    "    \"\"\"\n",
    "    Extract the <Band> or <Variable> name from the directory structure:\n",
    "    DATA_SV/Hima/<Band>/<Year>/<Month>/<Day>/<File>\n",
    "    \"\"\"\n",
    "    # Split the file path relative to the root directory\n",
    "    relative_path = os.path.relpath(file_path, root_dir)\n",
    "    parts = relative_path.split(os.sep)  # Split by folder separators\n",
    "\n",
    "    # The band/variable name will always be the first part after the root directory\n",
    "    # print(parts[0])\n",
    "    return parts[0]  # <Band> or <Variable>\n",
    "\n",
    "def index_files_by_timestamp_and_feature(root_dir, parse_fn):\n",
    "    \"\"\"\n",
    "    Index files by their timestamp and feature name for fast lookup.\n",
    "    Example: {timestamp: {'B04B': 'path/to/B04B_YYYYMMDDHHMM.tif', ...}}\n",
    "    \"\"\"\n",
    "    file_dict = {}\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.tif'):\n",
    "                timestamp = parse_fn(file)\n",
    "                feature_name = extract_band_or_variable_name(root, root_dir)  # Extract band/variable\n",
    "\n",
    "                if timestamp not in file_dict:\n",
    "                    file_dict[timestamp] = {}\n",
    "                file_dict[timestamp][feature_name] = os.path.join(root, file)\n",
    "\n",
    "    return file_dict\n",
    "\n",
    "def process_data(hima_dir, era5_dir, aws_dir):\n",
    "    \"\"\"Process all files and extract features and labels.\"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    # Index Hima and ERA5 files by timestamp and feature\n",
    "    print(\"Indexing Hima files...\")\n",
    "    hima_files = index_files_by_timestamp_and_feature(hima_dir, parse_timestamp_from_filename_hima)\n",
    "\n",
    "    print(\"Indexing ERA5 files...\")\n",
    "    era5_files = index_files_by_timestamp_and_feature(era5_dir, parse_timestamp_from_filename_era5_aws)\n",
    "\n",
    "    # Collect all possible feature names across all timestamps\n",
    "    all_hima_features = set()  # Unique Hima band names\n",
    "    all_era5_features = set()  # Unique ERA5 variable names\n",
    "\n",
    "    for files in hima_files.values():\n",
    "        all_hima_features.update(files.keys())\n",
    "    for files in era5_files.values():\n",
    "        all_era5_features.update(files.keys())\n",
    "\n",
    "    # Ensure a consistent list of feature names across all rows\n",
    "    all_features = list(all_hima_features) + list(all_era5_features)\n",
    "\n",
    "    # Traverse AWS files and extract labels and features\n",
    "    aws_files = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(aws_dir)\n",
    "        for file in files if file.endswith('.tif')\n",
    "    ]\n",
    "\n",
    "    for aws_path in tqdm(aws_files, desc=\"Processing AWS files\"):\n",
    "        aws_data = extract_image_data(aws_path)\n",
    "        aws_timestamp = parse_timestamp_from_filename_era5_aws(os.path.basename(aws_path))\n",
    "\n",
    "        # Find rows and cols where AWS stations exist (non-zero values)\n",
    "        rows, cols = np.where(aws_data > 0)  # Station locations\n",
    "\n",
    "        for row, col in zip(rows, cols):\n",
    "            aws_value = aws_data[row, col]\n",
    "\n",
    "            # Extract Himawari features for this timestamp and location\n",
    "            hima_features = {name: np.nan for name in all_hima_features}  # Default NaN\n",
    "            if aws_timestamp in hima_files:\n",
    "                for band_name, band_path in hima_files[aws_timestamp].items():\n",
    "                    hima_data = extract_image_data(band_path)\n",
    "                    hima_features[band_name] = hima_data[row, col]\n",
    "\n",
    "            # Extract ERA5 features for this timestamp and location\n",
    "            era5_features = {name: np.nan for name in all_era5_features}  # Default NaN\n",
    "            if aws_timestamp in era5_files:\n",
    "                for var_name, var_path in era5_files[aws_timestamp].items():\n",
    "                    era5_data = extract_image_data(var_path)\n",
    "                    era5_features[var_name] = era5_data[row, col]\n",
    "\n",
    "            # Store the data as a tuple (timestamp, row, col, aws_value, all features)\n",
    "            feature_tuple = (\n",
    "                aws_timestamp, row, col, aws_value,\n",
    "                *hima_features.values(),\n",
    "                *era5_features.values()\n",
    "            )\n",
    "            dataset.append(feature_tuple)\n",
    "\n",
    "    # Create column names dynamically\n",
    "    feature_names = ['timestamp', 'row', 'col', 'aws_value'] + all_features\n",
    "\n",
    "    # Convert dataset to DataFrame\n",
    "    df = pd.DataFrame(dataset, columns=feature_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "hima_dir = 'DATA_SV_ver2/DATA_SV/Hima'\n",
    "era5_dir = 'DATA_SV_ver2/DATA_SV/ERA5'\n",
    "aws_dir = 'DATA_SV_ver2/DATA_SV/Precipitation/AWS'\n",
    "# output_csv = 'aligned_data.csv'\n",
    "\n",
    "df = process_data(hima_dir, era5_dir, aws_dir)\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "df.to_csv('rainfall_dataset_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "row\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "col\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "aws_value\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "B09B\n",
      "False    81866\n",
      "True      3891\n",
      "Name: count, dtype: int64\n",
      "VSB\n",
      "True     44399\n",
      "False    41358\n",
      "Name: count, dtype: int64\n",
      "B16B\n",
      "False    81864\n",
      "True      3893\n",
      "Name: count, dtype: int64\n",
      "B06B\n",
      "True     49449\n",
      "False    36308\n",
      "Name: count, dtype: int64\n",
      "B14B\n",
      "False    81864\n",
      "True      3893\n",
      "Name: count, dtype: int64\n",
      "I4B\n",
      "False    80795\n",
      "True      4962\n",
      "Name: count, dtype: int64\n",
      "I2B\n",
      "False    81864\n",
      "True      3893\n",
      "Name: count, dtype: int64\n",
      "B05B\n",
      "True     45647\n",
      "False    40110\n",
      "Name: count, dtype: int64\n",
      "B10B\n",
      "False    81866\n",
      "True      3891\n",
      "Name: count, dtype: int64\n",
      "WVB\n",
      "False    81862\n",
      "True      3895\n",
      "Name: count, dtype: int64\n",
      "B11B\n",
      "False    81866\n",
      "True      3891\n",
      "Name: count, dtype: int64\n",
      "IRB\n",
      "False    81864\n",
      "True      3893\n",
      "Name: count, dtype: int64\n",
      "B04B\n",
      "True     44493\n",
      "False    41264\n",
      "Name: count, dtype: int64\n",
      "B12B\n",
      "False    81866\n",
      "True      3891\n",
      "Name: count, dtype: int64\n",
      "PEV\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "ISOR\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "R500\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "V850\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "IE\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "SLOR\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "R850\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "KX\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "CIN\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "CAPE\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "U850\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "V250\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "R250\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "EWSS\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "SSHF\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "TCLW\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "U250\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "TCWV\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "TCW\n",
      "False    85757\n",
      "Name: count, dtype: int64\n",
      "SLHF\n",
      "False    85757\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('rainfall_dataset.csv')\n",
    "for col in df.columns:\n",
    "    print(df[col].isnull().value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 - No Nulls: (34778, 38)\n",
      "Dataset 2 - Filled Missing Values: (85757, 34)\n",
      "Dataset 3 - Reduced Features with No Nulls: (80792, 34)\n",
      "Datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_csv('rainfall_dataset.csv')\n",
    "\n",
    "# Identify the protected features\n",
    "protected_features = ['timestamp', 'row', 'col', 'aws_value']\n",
    "\n",
    "# 1. Remove all rows with any null value (including non-protected features)\n",
    "df_no_nulls = df.dropna()\n",
    "print(f\"Dataset 1 - No Nulls: {df_no_nulls.shape}\")\n",
    "\n",
    "# 2. Remove specified features and fill missing values with mean (excluding protected features)\n",
    "features_to_remove = ['VSB', 'B06B', 'B05B', 'B04B']\n",
    "df_reduced = df.drop(columns=features_to_remove, errors='ignore')\n",
    "\n",
    "# Separate protected and non-protected features\n",
    "non_protected_features = df_reduced.columns.difference(protected_features)\n",
    "df_non_protected = df_reduced[non_protected_features]\n",
    "\n",
    "# Fill missing values in non-protected features with their column-wise mean\n",
    "df_non_protected_filled = df_non_protected.fillna(df_non_protected.mean())\n",
    "\n",
    "# Combine the protected features with the filled non-protected features\n",
    "df_filled = pd.concat([df_reduced[protected_features], df_non_protected_filled], axis=1)\n",
    "print(f\"Dataset 2 - Filled Missing Values: {df_filled.shape}\")\n",
    "\n",
    "# 3. Remove specified features and drop rows with any remaining nulls (excluding protected features)\n",
    "df_reduced_no_nulls = df_reduced.dropna(subset=non_protected_features)\n",
    "print(f\"Dataset 3 - Reduced Features with No Nulls: {df_reduced_no_nulls.shape}\")\n",
    "\n",
    "# Save the datasets to CSV files\n",
    "df_no_nulls.to_csv('rainfall_dataset_no_nulls.csv', index=False)\n",
    "df_filled.to_csv('rainfall_dataset_filled.csv', index=False)\n",
    "df_reduced_no_nulls.to_csv('rainfall_dataset_reduced_no_nulls.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle outliers & Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers removed, data normalized, and exported to 'cleaned_normalized_rainfall_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "df_no_nulls = pd.read_csv('datasets/rainfall_dataset_reduced_no_nulls.csv')\n",
    "\n",
    "# Exclude 'row' and 'col' from outlier detection and normalization\n",
    "features_to_process = [col for col in df_no_nulls.columns if col not in ['timestamp', 'row', 'col', 'aws_value']]\n",
    "\n",
    "# Function to detect and remove outliers using the IQR method\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Remove outliers\n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Apply outlier removal\n",
    "df_cleaned = remove_outliers_iqr(df_no_nulls, features_to_process)\n",
    "\n",
    "# Normalize features excluding 'row' and 'col'\n",
    "scaler = MinMaxScaler()\n",
    "df_cleaned[features_to_process] = scaler.fit_transform(df_cleaned[features_to_process])\n",
    "\n",
    "# Export the cleaned and normalized dataset to CSV\n",
    "df_cleaned.to_csv('datasets/cleaned_normalized_rainfall_data.csv', index=False)\n",
    "\n",
    "print(\"Outliers removed, data normalized, and exported to 'cleaned_normalized_rainfall_data.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
